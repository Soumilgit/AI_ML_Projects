{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ijGzTHJJUCPY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkHPv2myT2cx"
   },
   "source": [
    "## Overview\n",
    "\n",
    "### Gemini\n",
    "\n",
    "Gemini is a family of generative AI models developed by Google DeepMind that is designed for multimodal use cases.\n",
    "\n",
    "### Calling functions from Gemini\n",
    "\n",
    "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrkcqHrrwMAo"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "In this , we will learn how to use the Vertex AI Gemini API with the Vertex AI SDK for Python to make function calls via the Gemini 1.5 Pro (`gemini-1.5-pro`) model.\n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "- Install the Vertex AI SDK for Python\n",
    "- Use the Vertex AI Gemini API to interact with the Gemini 1.5 Pro (`gemini-1.5-pro`) model:\n",
    "- Use Function Calling in a chat session to answer user's questions about products in the Google Store\n",
    "- Use Function Calling to geocode addresses with a maps API\n",
    "- Use Function Calling for entity extraction on raw logging data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r11Gu7qNgx1p"
   },
   "source": [
    "## Getting Started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Vertex AI SDK for Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.7' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade --user --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart current runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XRvKdaPDTznN",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "NyKGtVQjgx13",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"qwiklabs-gcp-01-32be716382e6\"  # @param {type:\"string\"}\n",
    "LOCATION = \"europe-west1\"  # @param {type:\"string\"}\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92e02c3e0375"
   },
   "source": [
    "## Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXHfaVS66_01"
   },
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "lslYAvw37JGQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from vertexai.generative_models import (\n",
    "    FunctionDeclaration,\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Part,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28f36bd968b4"
   },
   "source": [
    "### Chat example: Using Function Calling in a chat session to answer user's questions about the Google Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d28287bde87"
   },
   "source": [
    "In this example, you'll use Function Calling along with the chat modality in the Gemini model to help customers get information about products in the Google Store.\n",
    "\n",
    "You'll start by defining three functions: one to get product information, another to get the location of the closest stores, and one more to place an order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3d4ed7ccc094",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "fetch_product_details = FunctionDeclaration(\n",
    "    name=\"fetch_product_details\",\n",
    "    description=\"Retrieve the stock amount and identifier for a specific product\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"item_name\": {\"type\": \"string\", \"description\": \"Name of the product\"}\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "locate_nearest_store = FunctionDeclaration(\n",
    "    name=\"locate_nearest_store\",\n",
    "    description=\"Find the location of the nearest store\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\"region\": {\"type\": \"string\", \"description\": \"User's location\"}},\n",
    "    },\n",
    ")\n",
    "\n",
    "create_order_request = FunctionDeclaration(\n",
    "    name=\"create_order_request\",\n",
    "    description=\"Submit a request to place an order\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"item\": {\"type\": \"string\", \"description\": \"Name of the product\"},\n",
    "            \"delivery_address\": {\"type\": \"string\", \"description\": \"Shipping address for the order\"},\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Product details function renamed to 'fetch_product_details'\")\n",
    "print(\"Store location function renamed to 'locate_nearest_store'\")\n",
    "print(\"Order placement function renamed to 'create_order_request'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7d7319febd8"
   },
   "source": [
    "Note that function parameters are specified as a Python dictionary in accordance with the [OpenAPI JSON schema format](https://spec.openapis.org/oas/v3.0.3#schemawr).\n",
    "\n",
    "Define a tool that allows the Gemini model to select from the set of 3 functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4b2d1900730d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "retail_tool = Tool(\n",
    "    function_declarations=[\n",
    "        fetch_product_details,\n",
    "        locate_nearest_store,\n",
    "        create_order_request,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b3781f6fd83"
   },
   "source": [
    "Now you can initialize the Gemini model with Function Calling in a multi-turn chat session.\n",
    "\n",
    "You can specify the `tools` kwarg when initializing the model to avoid having to send this kwarg with every subsequent request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ef8c2d811321",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GenerativeModel(\n",
    "    \"gemini-1.5-pro-001\",\n",
    "    generation_config=GenerationConfig(temperature=0),\n",
    "    tools=[retail_tool],\n",
    ")\n",
    "chat = model.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9125f50076d8"
   },
   "source": [
    "*Note: The temperature parameter controls the degree of randomness in this generation. Lower temperatures are good for functions that require deterministic parameter values, while higher temperatures are good for functions with parameters that accept more diverse or creative parameter values. A temperature of 0 is deterministic. In this case, responses for a given prompt are mostly deterministic, but a small amount of variation is still possible.*\n",
    "\n",
    "We're ready to chat! Let's start the conversation by asking if a certain product is in stock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9556d1ebcc1f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function_call {\n",
       "  name: \"get_product_info\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"product_name\"\n",
       "      value {\n",
       "        string_value: \"Pixel 8 Pro\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query_product_availability = \"\"\"\n",
    "Are any polar bears there Pro in Norway?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response_message = chat.send_message(query_product_availability)\n",
    "\n",
    "\n",
    "response_content = response_message.candidates[0].content.parts[0]\n",
    "\n",
    "\n",
    "print(\"Query sent to check population of polar bears in Norway. Response received:\")\n",
    "print(response_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3111780745fc"
   },
   "source": [
    "The response from the Gemini API consists of a structured data object that contains the name and parameters of the function that Gemini selected out of the available functions.\n",
    "\n",
    "Since this notebook focuses on the ability to extract function parameters and generate function calls, you'll use mock data to feed synthetic responses back to the Gemini model rather than sending a request to an API server (not to worry, we'll make an actual API call in a later example!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0c3f7b5474da",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "polar_bear_api_response = {\"species\": \"Polar Bear\", \"location\": \"Norway\", \"population_status\": \"healthy\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d86f58489be"
   },
   "source": [
    "In reality, you would execute function calls against an external system or database using your desired client library or REST API.\n",
    "\n",
    "Now, you can pass the response from the (mock) API request and generate a response for the end user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bbc8135093d",
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.7' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    Part.from_function_response(\n",
    "        name=\"fetch_product_details\",\n",
    "        response={\n",
    "            \"content\": polar_bear_api_response,\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "186d7afafee9"
   },
   "source": [
    "Next, the user might ask where they can buy a different phone from a nearby store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0258f7777226",
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.7' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "What about in Canada? How many polar bears are there in Canada?\n",
    "\"\"\"\n",
    "\n",
    "response = chat.send_message(prompt)\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da19e8e5292c"
   },
   "source": [
    "Again, you get a response with structured data, and the Gemini model selected the `get_product_info` function. This happened since the user asked about the \"Pixel 8\" phone this time rather than the \"Pixel 8 Pro\" phone.\n",
    "\n",
    "Now you can build another synthetic payload that would come from an external API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fba8fb03a8f9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here you can use your preferred method to make an API request and get a response.\n",
    "# In this example, we'll use synthetic data to simulate a payload from an external API response.\n",
    "\n",
    "polar_bear_api_response = {\"species\": \"Polar Bear\", \"location\": \"Norway\", \"population_status\": \"healthy\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adc1530ec2b1"
   },
   "source": [
    "Again, you can pass the response from the (mock) API request back to the Gemini model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3d8728b830d0",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function_call {\n",
       "  name: \"get_store_location\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"location\"\n",
       "      value {\n",
       "        string_value: \"Mountain View, CA\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    Part.from_function_response(\n",
    "        name=\"fetch_product_details\",\n",
    "        response={\n",
    "            \"content\": api_response,\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "response.candidates[0].content.parts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d9318a4e3a1"
   },
   "source": [
    "Wait a minute! Why did the Gemini API respond with a second function call to `get_store_location` this time rather than a natural language summary? Look closely at the prompt that you used in this conversation turn a few cells up, and you'll notice that the user asked about a product -and- the location of a store.\n",
    "\n",
    "In cases like this when two or more functions are defined (or when the model predicts multiple function calls to the same function), the Gemini model might sometimes return back-to-back or parallel function call responses within a single conversation turn.\n",
    "\n",
    "This is expected behavior since the Gemini model predicts which functions it should call at runtime, what order it should call dependent functions in, and which function calls can be parallelized, so that the model can gather enough information to generate a natural language response.\n",
    "\n",
    "Not to worry! You can repeat the same steps as before and build another synthetic payload that would come from an external API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6e42c851753c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "polar_bear_habitat_response = {\n",
    "    \"species\": \"Polar Bear\",\n",
    "    \"habitat_location\": \"Svalbard, Norway\",\n",
    "    \"protection_status\": \"Protected Area\"\n",
    "}\n",
    "\n",
    "response_message = chat.send_message(\n",
    "    Part.from_function_response(\n",
    "        name=\"get_habitat_location\",  \n",
    "        response={\"content\": polar_bear_habitat_response},\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "response_content = response_message.candidates[0].content.parts[0]\n",
    "\n",
    "\n",
    "print(\"Query sent to check polar bear habitat location. Response received:\")\n",
    "print(response_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67182d3a1651"
   },
   "source": [
    "And you can pass the response from the (mock) API request back to the Gemini model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a33bf8fe3458",
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.7' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    Part.from_function_response(\n",
    "        name=\"locate_nearest_store\",\n",
    "        response={\n",
    "            \"content\": polar_bear_api_response,\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51376798e2d6"
   },
   "source": [
    "And send the payload from the external API call so that the Gemini API returns a natural language summary to the end user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74f6d8722928",
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.7' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/msys64/ucrt64/bin/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "polar_bear_order_response = {\n",
    "    \"order_status\": \"confirmed\",\n",
    "    \"order_number\": 78910,\n",
    "    \"delivery_estimate\": \"3 days\",\n",
    "    \"items\": [\"Polar Bear Conservation Kit\", \"Educational Booklet\"]\n",
    "}\n",
    "\n",
    "\n",
    "order_response_message = chat.send_message(\n",
    "    Part.from_function_response(\n",
    "        name=\"process_conservation_order\", \n",
    "        response={\"content\": polar_bear_order_response},\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "order_response_content = order_response_message.candidates[0].content.parts[0]\n",
    "\n",
    "\n",
    "print(\"Order placed for polar bear conservation materials. Response received:\")\n",
    "print(order_response_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_function_calling.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
